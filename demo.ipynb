{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple hierarchical search\n",
    "\n",
    "Requires following steps:\n",
    "\n",
    "- chunk documents\n",
    "- generate embeddings\n",
    "- set up db table and sqlite-vss\n",
    "- populate db table\n",
    "- test queries\n",
    "- simple UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk documents\n",
    "As I already have tools for working with Sphinx XML, and given that this work is partly related to developing Jupyter Book / Sphinx tools to work with OU-XML, let's start there..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up, we need to be be able to chunk markdown documents. We can render Jupyter notebook `.ipnyb` documents, as well as MyST markdown, into Sphinx-XML by building using `jb build ./tests/book --all --builder custom --custom-builder xml` (I really need to do some digging to find the Python calls used by the command-line command, rather than having to use the CLI.)\n",
    "\n",
    "Let's start with a hack to just convert a single file. Do this by creating a dummy `toc.yml` file and running the builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<section classes=\"tex2jax_ignore mathjax_ignore\" docname=\"dummy\" header_level=\"1\" ids=\"lots-of-interesting-facts-about-snafu\" names=\"lots\\ of\\ interesting\\ facts\\ about\\ snafu\">\n",
      "  <title>Lots of Interesting Facts about Snafu</title>\n",
      "  <paragraph>The capital of Baswas is foobar.</paragraph>\n",
      "  <section docname=\"dummy\" header_level=\"2\" ids=\"a-history-of-junkwap\" names=\"a\\ history\\ of\\ junkwap\">\n",
      "    <title>A history of <emphasis>Junkwap</emphasis></title>\n",
      "    <paragraph>Junkwap was founded in 1874. It is mainly green and blue.</paragraph>\n",
      "    <paragraph>Here is a list:</paragraph>\n",
      "    <bullet_list bullet=\"-\">\n",
      "      <list_item>\n",
      "        <paragraph>one item</paragraph>\n",
      "      </list_item>\n",
      "      <list_item>\n",
      "        <paragraph>another item</paragraph>\n",
      "      </list_item>\n",
      "    </bullet_list>\n",
      "    <paragraph>The sun never wrinkles.</paragraph>\n",
      "    <section docname=\"dummy\" header_level=\"3\" ids=\"the-asvrograde-of-junkwap\" names=\"the\\ asvrograde\\ of\\ junkwap\">\n",
      "      <title>The asvrograde of <emphasis>Junkwap</emphasis></title>\n",
      "      <paragraph>The asvrograde of Junkwap is quite fruity and smells mainly of cheese.</paragraph>\n",
      "      <section docname=\"dummy\" dupnames=\"a\\ part\" header_level=\"4\" ids=\"a-part\">\n",
      "        <title>A part</title>\n",
      "        <paragraph>Some bits</paragraph>\n",
      "      </section>\n",
      "    </section>\n",
      "    <section docname=\"dummy\" header_level=\"4\" ids=\"the-oblograde-of-junkway\" names=\"the\\ oblograde\\ of\\ junkway\">\n",
      "      <title>The oblograde of Junkway</title>\n",
      "      <paragraph>The oblograde of Junkwap is a round square whose in a world with a really strange geometry where the internal angles of the round square, which is a triangle, add up to 243 degrees.</paragraph>\n",
      "      <section docname=\"dummy\" dupnames=\"a\\ part\" header_level=\"5\" ids=\"id1\">\n",
      "        <title>A part</title>\n",
      "        <paragraph>Some more bits</paragraph>\n",
      "      </section>\n",
      "    </section>\n",
      "  </section>\n",
      "  <section docname=\"dummy\" header_level=\"3\" ids=\"the-decline-of-junkwap\" names=\"the\\ decline\\ of\\ junkwap\">\n",
      "    <title>The Decline of Junkwap</title>\n",
      "    <paragraph>Everything went wrong, mainly with the railway bikes, which kept falling off the edge of piebald feint lined moleskin book.</paragraph>\n",
      "    <paragraph>And then it began to snow, and all the wurbs were shnorked.</paragraph>\n",
      "  </section>\n",
      "  <compound caption=\"True\" classes=\"toctree-wrapper\" docname=\"dummy\">\n",
      "        </compound>\n",
      "</section>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pip install lxml\n",
    "\n",
    "import subprocess\n",
    "from lxml import etree\n",
    "from pathlib import Path\n",
    "\n",
    "_ = subprocess.run(\n",
    "    \"jb build . --all --builder custom --custom-builder xml\",\n",
    "    shell=True,\n",
    "    cwd=\".\",\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "# We could perhaps get the name part of the filename\n",
    "# by parsing the _toc.yml file.\n",
    "xml_out = Path(\".\") / \"_build\" / \"xml\" / f\"dummy.xml\"\n",
    "\n",
    "\n",
    "def create_top_section_root(xml_out):\n",
    "    tree = etree.parse(\n",
    "        xml_out, parser=etree.XMLParser(strip_cdata=False, remove_blank_text=True)\n",
    "    )\n",
    "    etree.cleanup_namespaces(tree)\n",
    "    root = tree.getroot().find(\"section\")\n",
    "    # result = root.find(\".//title\").getnext()\n",
    "    return tree, root\n",
    "\n",
    "\n",
    "tree, root = create_top_section_root(xml_out)\n",
    "\n",
    "xml_bytes = etree.tostring(root, pretty_print=True, encoding=\"utf-8\")\n",
    "\n",
    "print(xml_bytes.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection of the generated XML shows:\n",
    "\n",
    "- a root `<document>` element with a `source` attribute giving the path to the source markdown file;\n",
    "- at each markdown heading, a new `<section>` tag/block is introduced that includes the following attributes:\n",
    "    - `header_level`: the header level is also the section level / depth;\n",
    "    - `names`: an escaped, decased version of the section heading;\n",
    "    - `ids`: a within document identifier generated from the section heading; this name not be unique across different documents; within a document, if a section heading is duplicatesd, the first `ids` is set in the normal way, and a new id created for duplicates; a `dupnames` attribute is also set for all duplicated sections in the `<section>` tag that contains the duplicated escaped `names` value;\n",
    "- the first child of the section block is a `<title>` tag; this tag contains the heading text (that is, the section heading);\n",
    "- implicitly, each section contains its subsections;\n",
    "- sections contain `<paragraph>` elements and other block level chunks, such as lists. Within list elements, we also also have paragraphs, so paragtaphs may provide a useful level of chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating section chunks\n",
    "\n",
    "The following chunking strategy recursively iterates through each section in a provided Sphinx-XML document.\n",
    "\n",
    "Each chunk is flattened to simple, unformatted, text which can be used to generate embeddings or as part of an LLM prompt.\n",
    "\n",
    "Paragraph level chunks are also extracted and flattened within each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lots of Interesting Facts about Snafu .', 'A history of  Junkwap .', 'The asvrograde of  Junkwap .', 'A part .', 'The oblograde of Junkway .', 'A part .', 'The Decline of Junkwap .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['a-history-of-junkwap', 'the-asvrograde-of-junkwap', 'a-part', 'the-oblograde-of-junkway', 'id1', 'the-decline-of-junkwap'])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ou_xml_validator.utils import flatten_to_text\n",
    "\n",
    "# Restate the root definition here so we can\n",
    "# rerun this cell on a pristine root element\n",
    "tree, root = create_top_section_root(xml_out)\n",
    "\n",
    "sections = {}\n",
    "\n",
    "\n",
    "def post_stop(text):\n",
    "    \"\"\"Add a trailing period/full stop if not exists.\"\"\"\n",
    "    return f\"{text}.\" if not text.strip().endswith(\".\") else text.strip()\n",
    "\n",
    "# Simple repair script for titles so they end with a .\n",
    "# This is useful for sentence chunking\n",
    "# There is an issue that if the content preceding the title\n",
    "# has no \".\", we don't get a sentence break.\n",
    "title_tags = root.xpath(\"//title\")\n",
    "for title_tag in title_tags:\n",
    "    if not title_tag.text.strip().endswith(\".\"):\n",
    "        period_node = etree.Element(\"text\")\n",
    "        period_node.text = \".\"\n",
    "        title_tag.append(period_node)\n",
    "\n",
    "def chunk_documents(root, sections=None):\n",
    "    sections = {} if sections == None else sections\n",
    "    # Find each section within the current section\n",
    "    for section in root.findall(\"./section\"):\n",
    "        section_id = section.get(\"ids\")\n",
    "        sections[section_id] = {}\n",
    "        section_ = sections[section_id]\n",
    "        section_[\"title\"] = flatten_to_text(section.find(\"./title\")).strip()\n",
    "\n",
    "        # Find paragraphs in the section and flatten each one\n",
    "        # Paragraphs should not be in a child section\n",
    "        # Relative path given by:\n",
    "        # tree.getpath(p)[len(tree.getpath(section)) :]\n",
    "        section_[\"paragraph_text\"] = [\n",
    "            post_stop(flatten_to_text(p))\n",
    "            for p in section.findall(\".//paragraph\")\n",
    "            if \"section\" not in tree.getpath(p)[len(tree.getpath(section)) :]\n",
    "        ]\n",
    "\n",
    "        # For lists, should we also\n",
    "        # group all the items as a single paragraph?\n",
    "\n",
    "        # Flatten text in section\n",
    "        section_[\"full_flat_text\"] = post_stop(flatten_to_text(section))\n",
    "\n",
    "        section_[\"subsections\"] = section.findall(\"./section\")\n",
    "        # Flatten text in a subsection\n",
    "        section_[\"flat_subsections_text\"] = [\n",
    "            post_stop(flatten_to_text(s)) for s in section_[\"subsections\"]\n",
    "        ]\n",
    "\n",
    "        # Recurse through subsections\n",
    "        chunk_documents(section, sections)\n",
    "\n",
    "        # Convert section refernce to id\n",
    "        section_[\"subsections\"] = [s.get(\"ids\") for s in section_[\"subsections\"]]\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "sections = chunk_documents(root)\n",
    "\n",
    "sections.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'A history of  Junkwap .',\n",
       " 'paragraph_text': ['Junkwap was founded in 1874. It is mainly green and blue.',\n",
       "  'Here is a list:.',\n",
       "  'one item.',\n",
       "  'another item.',\n",
       "  'The sun never wrinkles.'],\n",
       " 'full_flat_text': ' A history of  Junkwap . Junkwap was founded in 1874. It is mainly green and blue. Here is a list:   one item  another item The sun never wrinkles.  The asvrograde of  Junkwap . The asvrograde of Junkwap is quite fruity and smells mainly of cheese.  A part . Some bits  The oblograde of Junkway . The oblograde of Junkwap is a round square whose in a world with a really strange geometry where the internal angles of the round square, which is a trianle, add up to 243 degrees.  A part . Some more bits.',\n",
       " 'subsections': ['the-asvrograde-of-junkwap', 'the-oblograde-of-junkway'],\n",
       " 'flat_subsections_text': [' The asvrograde of  Junkwap . The asvrograde of Junkwap is quite fruity and smells mainly of cheese.  A part . Some bits.',\n",
       "  ' The oblograde of Junkway . The oblograde of Junkwap is a round square whose in a world with a really strange geometry where the internal angles of the round square, which is a trianle, add up to 243 degrees.  A part . Some more bits.']}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[\"a-history-of-junkwap\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each section, we can now generate embeddings on:\n",
    "\n",
    "- the section title;\n",
    "- the paragraph text that is not contained within a child section element; (this is available at paragraph level but could be joined);\n",
    "- the full flat text of the whole section (including flattened child sections);\n",
    "- the flat text of each child section.\n",
    "\n",
    "We can generate embeddings using [`sentence-transformers`](https://www.sbert.net/docs/pretrained_models.html), although we are limited to sentences of about 300-400 words. For many paragraphs, this will be okay, but longer paragraphs will either need truncating or aggregating. For example, we could return an embedding for a long paragraph by aggregating over embeddings of particular chunks, either by creating a single aggregate embedding, such as a mean embedding, or in a search strategy that matches using the best match over all partial paragraph embeddings associated with a paragraph.\n",
    "\n",
    "For long paragraphs, we could use something liuke the `spacy` sentence chunker to chunk our sentences and then build up chinks that as are long as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([ A history of  Junkwap .,\n",
       "  Junkwap was founded in 1874.,\n",
       "  It is mainly green and blue.,\n",
       "  Here is a list:   one item  another item The sun never wrinkles.  ,\n",
       "  The asvrograde of  Junkwap .,\n",
       "  The asvrograde of Junkwap is quite fruity and smells mainly of cheese.  ,\n",
       "  A part .,\n",
       "  Some bits  The oblograde of Junkway .,\n",
       "  The oblograde of Junkwap is a round square whose in a world with a really strange geometry where the internal angles of the round square, which is a trianle, add up to 243 degrees.  ,\n",
       "  A part . Some more bits.],\n",
       " [ A history of  Junkwap .,\n",
       "  Junkwap was founded in 1874.,\n",
       "  It is mainly green and blue.,\n",
       "  Here is a list:   one item,\n",
       "   another item The sun never wrinkles.,\n",
       "   ,\n",
       "  The asvrograde of  Junkwap .,\n",
       "  The asvrograde of Junkwap is quite fruity,\n",
       "  and smells mainly of cheese.  ,\n",
       "  A part .,\n",
       "  Some bits  The oblograde of Junkway .,\n",
       "  The oblograde of Junkwap is a round square whose,\n",
       "  in a world with a really strange geometry where,\n",
       "  the internal angles of the round square, which,\n",
       "  is a trianle, add up to 243 degrees,\n",
       "  .  ,\n",
       "  A part . Some more bits.])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install spacy\n",
    "# Install simple model for sentence splitting\n",
    "# import spacy.cli\n",
    "# spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "import spacy\n",
    "import math\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = sections[\"a-history-of-junkwap\"][\"full_flat_text\"]\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "def paragraph_chunks(text, maxlen=350):\n",
    "    \"\"\"Generate chunks around paragraphs and sentences.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    chunk_start_sent = sentences[0]\n",
    "    chunks = [chunk_start_sent]\n",
    "    for sent in sentences[1:]:\n",
    "        chunk_len = len(chunks[-1])\n",
    "        next_sent_len = len(sent)\n",
    "        proposed_len = chunk_len + next_sent_len\n",
    "        if proposed_len < maxlen:\n",
    "            chunks[-1] = doc[chunk_start_sent.start : sent.end]\n",
    "        else:\n",
    "            chunks.append(sent)\n",
    "            chunk_start_sent = sent\n",
    "\n",
    "    # Cap the length of chunks, if necesary by splitting paragraphs\n",
    "    newchunks = []\n",
    "    for chunk in chunks:\n",
    "        chunk_len = len(chunk)\n",
    "        if chunk_len > maxlen:\n",
    "            # If a chunk is overlong, split it into N equal size partial chunks,\n",
    "            # for smallest N where partial chunk size < maxlen\n",
    "            chunk_block = int( chunk_len / math.ceil(chunk_len / maxlen))\n",
    "            for i in range(0, len(chunk), chunk_block):\n",
    "                newchunks.append(chunk[i : i + chunk_block])\n",
    "        else:\n",
    "            newchunks.append(chunk)\n",
    "    #chunks = [c.text for c in newchunks]\n",
    "    return chunks, newchunks\n",
    "\n",
    "\n",
    "paragraph_chunks(text, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating embeddings\n",
    "\n",
    "One of the simplest way of generating embeddings is to use the Python [`sentence-transformers` package](https://www.sbert.net/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.5472, 0.6330, 0.6330]])\n"
     ]
    }
   ],
   "source": [
    "# %pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# The model will be automatically downloaded if required\n",
    "SENTENCE_TRANSFORMER_MODEL = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)\n",
    "\n",
    "# The default cut off is length 128 tokens\n",
    "MAX_SENTENCE_TRANSFORMER_TOKENS = 500\n",
    "\n",
    "# Max length is 510, but there be overhead...\n",
    "MAX_SENTENCE_TRANSFORMER_TOKENS = min(MAX_SENTENCE_TRANSFORMER_TOKENS, 300)\n",
    "model.max_seq_length = MAX_SENTENCE_TRANSFORMER_TOKENS\n",
    "\n",
    "_query = \"How big is London\"\n",
    "_responses = [\n",
    "    \"London has 9,787,426 inhabitants at the 2011 census\",\n",
    "    \"London is known for its finacial district\",\n",
    "    \"London is known for its finacial district\",\n",
    "]\n",
    "\n",
    "# Generate embdding for one record\n",
    "query_embedding = model.encode(_query)\n",
    "# Generate emmbddings for mutliple records\n",
    "passage_embedding = model.encode(_responses)\n",
    "\n",
    "#Find match score between query embedding and each response embedding\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding vector returned by the sentence tranformer is a `numpy.ndarray`. Mean aggregate embeddings can be generated by using `np.mean(INDIVIDUAL_EMBEDDINGS, axis=0)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create ene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector search\n",
    "\n",
    "We can search across the generated embeddings using a SQLite3 database using the [`sqlite-vss`](https://github.com/asg017/sqlite-vss) extension installed [[about](https://observablehq.com/@asg017/introducing-sqlite-vss)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tonyhirst/Documents/GitHub/hierarchical-search/demo.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tonyhirst/Documents/GitHub/hierarchical-search/demo.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m db_ \u001b[39m=\u001b[39m sqlite3\u001b[39m.\u001b[39mconnect(\u001b[39m\"\u001b[39m\u001b[39m:memory:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tonyhirst/Documents/GitHub/hierarchical-search/demo.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m db_\u001b[39m.\u001b[39menable_load_extension(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tonyhirst/Documents/GitHub/hierarchical-search/demo.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m sqlite_vss\u001b[39m.\u001b[39mload(db)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tonyhirst/Documents/GitHub/hierarchical-search/demo.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m (version,) \u001b[39m=\u001b[39m db_\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mselect vss_version()\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mfetchone()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tonyhirst/Documents/GitHub/hierarchical-search/demo.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m version\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "# Install sqlite_utils, sqlite-vss \n",
    "#%pip install --upgrade sqlite_utils sqlite-vss sqlite-utils-sqlite-vss\n",
    "\n",
    "# MacOS extension installation  :\n",
    "# https://til.simonwillison.net/sqlite/sqlite-extensions-python-macos\n",
    "\n",
    "# Check install\n",
    "import sqlite3\n",
    "import sqlite_vss\n",
    "\n",
    "# Manually enable extension\n",
    "db_ = sqlite3.connect(\":memory:\")\n",
    "db_.enable_load_extension(True)\n",
    "sqlite_vss.load(db_)\n",
    "\n",
    "(version,) = db_.execute(\"select vss_version()\").fetchone()\n",
    "version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a `sqlite_utils.Database` with the `sqlite_vss` extension loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Database <sqlite3.Connection object at 0x106de2890>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlite_utils import Database\n",
    "\n",
    "# Create the database\n",
    "# db = Database(memory=True)\n",
    "db = Database(\"hsearch_demo.db\", recreate=True)\n",
    "\n",
    "# Enable the extension\n",
    "db.conn.enable_load_extension(True)\n",
    "sqlite_vss.load(db.conn)\n",
    "\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible structure for an appropriate database table is as follows:\n",
    "\n",
    "- `full_text`: the markdown text of the whole chunk;\n",
    "- `raw_embedding`: an embedding for the whole chunk;\n",
    "- `agg_embedding`: an aggregate embedding that is also a function of the raw embedding of child chunks;\n",
    "- `doc_id`: for example, based on the document path;\n",
    "- `section_id`: this could be the section identifier; it will only be guaranteed unique across documents if combined with the document path or identifier, for example;\n",
    "- `parent_section_id`: the identifier of the parent section;\n",
    "- `child_section_ids`: a list of child section identifiers; to reconstruct full text from section parts, we would need to have \"top level\" text and the section references in the correct locations within that text;\n",
    "- `heading`: the section heading;\n",
    "- `heading_embedding`: an embedding generated from the heading text;\n",
    "- `section_level`: for example, the `header_level` in the Sphinx generated XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sqlite_utils.Database` is easier to work with. For example, we can triviually create a table on the database:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
